# Optimization Projects

## Key Concepts Learned:
- **Stochastic Gradient Descent (SGD)**  
  An optimization algorithm that updates model parameters using a single training sample or a small batch at a time to speed up learning.

- **Momentum**  
  A technique that accelerates gradient descent by incorporating a fraction of the previous update's direction to dampen oscillations.

- **Adagrad**  
  An adaptive learning rate method that adjusts the learning rate for each parameter based on the historical gradients, making it well-suited for sparse data.

- **RMSprop**  
  An adaptive learning rate algorithm that divides the gradient by a moving average of its squared values to stabilize updates and improve convergence.

- **Adam (Adaptive Moment Estimation)**  
  A powerful optimization algorithm that combines the benefits of Momentum and RMSprop by using both first and second moments of the gradients.

- **Mini-Batch Gradient Descent**  
  A variant of gradient descent that divides the training data into small batches, balancing the speed of stochastic gradient descent and the stability of batch gradient descent.
